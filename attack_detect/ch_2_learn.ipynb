{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 과정\n",
    "\n",
    "데이터를 정수형으로 반환했으면 이진분류를 통해 학습하여 보자.\n",
    "\n",
    "---\n",
    "## (1)class Model\n",
    "\n",
    "모델 클래스에서는 필요한 가중치와 계산에 필요한 함수들로 구성된다.\n",
    "\n",
    "이진 분류의 핵심 계산함수(activate_function)는 sigmoid 함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.weight = np.zeros((3, 1))\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        feature_matrix = x\n",
    "        z = feature_matrix @ self.weight\n",
    "        pred = self.activate(z)\n",
    "        return pred\n",
    "    \n",
    "    def activate(self, x):\n",
    "        y = 1/ (1+np.exp(-x))\n",
    "        return y\n",
    "    \n",
    "    def update(self, weight):\n",
    "        self.weight = weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)class MyOptim\n",
    "\n",
    "myoptim 클래스에서는 학습률과 정규화 상수를 추가한다.\n",
    "\n",
    "학습률을 가중치의 변화량에 영향을 준다.\n",
    "\n",
    "또한 손실값과 잔차를 계산하여 최대 손실의 방향으로 (gradient)로 이동하여 학습한다.\n",
    "\n",
    "이때 학습이 잘 되는지 확인하기 위해 손실값을 반환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOptim:\n",
    "    def __init__(self, model):\n",
    "        self.model  = model\n",
    "        self.eps    = 1e-8\n",
    "        self.lr = 0.1\n",
    "    def compute_loss(self, label, prediction):\n",
    "        # ===========================\n",
    "        eps = self.eps\n",
    "        loss = (1 / (len(label))) * ((((-1) * label.T) @ np.log(prediction + eps)) - ((np.ones_like(label) - label).T @ np.log(np.ones_like(label) - prediction + eps)))\n",
    "        # ===========================\n",
    "        return loss\n",
    "     \n",
    "    def compute_residual(self, label, prediction):\n",
    "        # ===========================\n",
    "        res = label - prediction\n",
    "        # ===========================\n",
    "        return res\n",
    "   \n",
    "    def compute_gradient(self, x, label, pred):\n",
    "        # ===========================\n",
    "        A = x\n",
    "        grad = (1 / (len(label))) * (A.T @ (pred - label))\n",
    "        # ===========================\n",
    "        return grad\n",
    "        \n",
    "    def step(self, grad):\n",
    "        # ===========================\n",
    "        weight = self.model.weight\n",
    "        next_weight = weight - (self.lr * grad)\n",
    "        self.model.update(next_weight)\n",
    "        pass \n",
    "        # ==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) test accuracy\n",
    "\n",
    "이진 분류는 기준점 (여기에선 0.5)을 기준으로 그 이상과 이하로 분류하는 모델이다.\n",
    "\n",
    "시그모이드 (activate function)을 통과한 결과값을 토대로 해당 샘플에 대해 값을 부여한다.\n",
    "\n",
    "따라서 학습을 통해 업데이트한 가중치를 이용하여, 테스트 데이터를 예측 모델에 대입한다.\n",
    "\n",
    "후에 0.5 이상인 데이터의 합 갯수를 구하여 전체 크기로 나누어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape\n",
    "\n",
    "test_pred = model.predict(x_test)\n",
    "test_pred_class = (test_pred >= 0.5).astype(int)\n",
    "\n",
    "correct = (test_pred_class.to_numpy().flatten() == y_test.flatten()).sum()\n",
    "total = len(y_test)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
