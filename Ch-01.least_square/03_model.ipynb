{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "학습에 대한 기본 과정을 이해하기 위해 선형대수학의 최소제곱해를 생각하자.\n",
    "\n",
    "현재 data는 (x,y)의 쌍이 100개 존재함을 앞에서 알 수 있었다.\n",
    "\n",
    "y가 정답이라고 하였을때, 우리는 정답 데이터를 통해 x를 정답에 가깝게 만들어주는 값을 찾고싶다.\n",
    "\n",
    "즉 y = ax와 같은 결과에서 x가 y에 가까워 질 수 있는 a 를 찾는 과정이다.\n",
    "\n",
    "이때 a를 parameter라고 하고 주로 theta로 작성한다.\n",
    "\n",
    "---\n",
    "\n",
    "그러나 시각화에서 처럼 데이터는 일반적으로 선형적으로 이루어져있지 않다.\n",
    "\n",
    "이를 위해 등장한 개념이 feature matrix와 activation function 과 같은 개념들이다.\n",
    "\n",
    "우선 feature matrix에 대해 알아보고, activation function은 후에 알아보도록 하겠다.\n",
    "\n",
    "feature matrix의 설계는 머신러닝과 딥러닝의 가장 핵심적인 차이라고 볼 수 있다.\n",
    "\n",
    "머신러닝에서는 feature matrix를 직접 설게하여 가장 적합한 feature을 찾는것이 중요하다.\n",
    "\n",
    "---\n",
    "\n",
    "우선 이 챕터에서는 최소제곱해를 통한 가장 기본적인 정답에 가까워지는 과정을 알아 볼것이다.\n",
    "\n",
    "    따라서 어떤 차수의 feature을 사용하는 것이 비선형적 곡선에 적합한 \n",
    "\n",
    "    feature가 되는지 차이를 알아보는것이 핵심이라고 할 수 있다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyUtil:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        pass\n",
    "    \n",
    "    def get_feature(self, x, p):\n",
    "        self.feature = np.column_stack([x ** j for j in range(p)])\n",
    "        #readme의 feature에서 p가 변수로 적용된다.\n",
    "        \n",
    "    def compute_regression_polynomial(self, x, y, p = 1, alpha = 0):\n",
    "        self.get_feature(x, p)\n",
    "        x = x.reshape(-1, 1)\n",
    "        y = y.reshape(-1, 1)\n",
    "        theta = self.find_theta(y)\n",
    "        f_hat, loss = self.get_loss(y, theta)\n",
    "        return f_hat, loss\n",
    "        #feature을 불러오고, f_hat과 y의 차이를 계산한다. 이 loss값은 실제로 줄어드는지에 대해 확인하기 위해 사용된다.\n",
    "        #또한, p에 따른 theta를 찾는다.\n",
    "\n",
    "    def find_fhat(self, theta):\n",
    "        f_hat = self.feature @ theta\n",
    "        return f_hat\n",
    "    \n",
    "    def get_loss(self, label, theta):\n",
    "        f_hat = self.find_fhat(theta)\n",
    "        res = label - f_hat\n",
    "        loss = (1 / (2 * len(label)) * (res.T @ res))\n",
    "        return f_hat, loss\n",
    "    \n",
    "    def find_theta(self, label):\n",
    "        theta = (np.linalg.inv(self.feature.T @ self.feature) @ self.feature.T) @ label\n",
    "        theta = theta.reshape(-1, 1)\n",
    "        return theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
